# ğŸ’¼ PaddleMono: ä¸€ä¸ªå…³äºå•ç›®æ·±åº¦ä¼°è®¡çš„ç»Ÿä¸€æ¡†æ¶
</div>

<div align="center">

[English](README.md)| [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

</div>
PaddleMonoæ˜¯ä¸€æ¬¾åŸºäº PaddlePaddle çš„å•ç›®æ·±åº¦ä¼°è®¡å·¥å…·ç®±ï¼Œæ˜¯ PaddleDepth é¡¹ç›®çš„æˆå‘˜ä¹‹ä¸€ã€‚
å®ƒå…·æœ‰å¯æ‰©å±•æ€§ï¼Œå®¹æ˜“ä¸Šæ‰‹çš„ç‰¹ç‚¹ï¼Œæ­¤å¤–å®ƒåœ¨ç›¸åŒçš„è®­ç»ƒç­–ç•¥å’Œç¯å¢ƒä¸‹å…¬å¹³æ¯”è¾ƒäº†å•ç›®æ·±åº¦ä¼°è®¡é¢†åŸŸé‡Œé¢SOTA(state-of-the-art)çš„ç®—æ³•

## ğŸ’¡ å¯è§†åŒ–

### monocular depth estimation
<div align="center">
    <img src="./assets/monocular.gif" width = "400" />
</div>

## ğŸ¦„ åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹åº“

ä½œä¸ºåˆå§‹ç‰ˆæœ¬ï¼ŒPaddldMonoç›®å‰æ”¯æŒä»¥ä¸‹ç®—æ³•ã€‚

[comment]: <> (- Monodepth2)

[comment]: <> (- MLDA-Net)

[comment]: <> (- Depth Hints &#40;ä»¥ä¸Šä¸¤ä¸ªæ¨¡å‹è®­ç»ƒæ—¶å‡å¯å¼€å¯Depth Hints&#41;)

[comment]: <> (- BTS)


1. [Monodepth2 (ICCV2019)[1]](configs/monodepthv2/README.md)
2. [MLDA-Net (TIP2021)[2]](configs/mldanet/README.md)
3. [Depth Hints (ICCV2019)[3]](configs/depth_hints/README.md)
4. [BTS[4]](configs/bts/README.md)

è¯·ç‚¹å‡»ä¸Šæ–¹çš„è¶…é“¾æ¥æŸ¥çœ‹æ¯ä¸ªç®—æ³•çš„å®ç°ç»†èŠ‚

## ğŸš€ å®‰è£…

ä½ å¯ä»¥é€šè¿‡å¦‚ä¸‹å‘½ä»¤ä¸‹è½½PaddleMonoå·¥å…·ç®±

```
git clone https://github.com/PaddlePaddle/PaddleDepth.git
cd Paddle-Mono
pip install -r requirements.txt
```
è¯·åœ¨Python 3.9ä¸­ä½¿ç”¨PaddleMono.

## ğŸ•ï¸ æ•°æ®é›†å‡†å¤‡
ä½ å¯ä»¥å‚ç…§ [dataset_prepare](data_prepare/data_prepare.md) æ¥è¿›è¡Œæ•°æ®é›†çš„å‡†å¤‡.

## ğŸ“ å¦‚ä½•ä½¿ç”¨


1. åœ¨ `configs` æ–‡ä»¶å¤¹ä¸­ä¿®æ”¹ç›¸åº”æ¨¡å‹çš„é…ç½®æ–‡ä»¶.
2. è¿è¡Œ `train.py` æ¥è®­ç»ƒconfigæŒ‡å®šçš„æ¨¡å‹, ä¾‹å¦‚: `CUDA_VISIBLE_DEVICES=0 python train.py --config configs/monodepthv2/mdp.yml`ï¼Œè‹¥ä½¿ç”¨å•æœºå¤šå¡ï¼Œåˆ™è¿è¡Œ`CUDA_VISIBLE_DEVICES=0,1,2,3 python -m paddle.distributed.launch train.py --config configs/monodepthv2/mdp.yml --num_gpus 4`

* æˆ‘ä»¬æä¾›shellè„šæœ¬æ¥å¸®åŠ©æ‚¨å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœ: `bash configs/monodepth/mdp.sh`.

## â™¨ï¸ å®šåˆ¶åŒ–ç®—æ³•

ä½ å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ­¥éª¤å¼€å‘è‡ªå·±çš„ç®—æ³•:

1. æ£€æŸ¥ä½ çš„æ¨¡å‹æ˜¯å¦éœ€è¦æ¨¡å‹æ¥è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœæœ‰æŠŠæ¨¡å‹åŠ å…¥åˆ° `model`ä¸­
2. åŠ å…¥ä½ è‡ªå·±çš„é…ç½®æ–‡ä»¶ï¼ˆ.sh æˆ– .ymlï¼‰

## â­ ç»“æœ

æˆ‘ä»¬åœ¨KITTIæ•°æ®é›†ä¸Šæ ¹æ®é€šç”¨çš„Eigenåˆ’åˆ†æ–¹æ³•è¯„æµ‹äº†PaddleMonoå·²ç»å®ç°çš„ç®—æ³•ã€‚

æ³¨æ„æˆ‘ä»¬å¹¶æ²¡æœ‰é€šè¿‡é¢å¤–çš„æŠ€å·§æ¥ä¼˜åŒ–Monodepth2æ¨¡å‹çš„ç»“æœï¼Œå› æ­¤ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨.shçš„è„šæœ¬æ–‡ä»¶æ¥å¤ç°æˆ‘ä»¬åœ¨è¡¨æ ¼ä¸­æŠ¥å‘Šçš„ç²¾åº¦ã€‚

[comment]: <> (å¯¹äºMLDA-Netï¼Œç›®å‰è¿˜æ²¡æœ‰å®Œå…¨å¯¹é½ï¼Œè¡¨ä¸­ç»™å‡ºtorchæƒé‡è½¬ä¸ºpaddleæƒé‡ä¹‹åçš„æµ‹è¯•ç²¾åº¦ã€‚)

### KITTI

|     Method        | abs_rel | sq_rel | rms | log_rms | a1  | a2  | a3 |
|-------------|-------|-------|-------|-------|--------|--------|---------|
| Monodepth2_640x192 | 0.112 | 0.839 | 4.846 | 0.193 | 0.875  | 0.957 | 0.980   |
| Monodepth2_1024x32 | 0.112 | 0.833 | 4.748 | 0.191 | 0.880  | 0.960 | 0.981   |
| Depth Hints_640x192 | 0.110 | 0.818 | 4.728 | 0.189 | 0.881  | 0.959 | 0.981   |
| Depth Hints_1024x320 | 0.109 | 0.794 | 4.474 | 0.185 | 0.887  | 0.963 | 0.982   |
| MLDANet_640x192 | 0.108 | 0.829 | 4.678 | 0.184 | 0.885  | 0.962 | 0.983   |
| BTS Densenet121_704x352 | 0.050 | 0.201 | 2.547 | 0.082 | 0.970  | 0.995 | 0.999   |

## ğŸŒŸ è´¡çŒ®

PaddleMonoå·¥å…·ç®±ç›®å‰è¿˜åœ¨ç§¯æç»´æŠ¤ä¸å®Œå–„è¿‡ç¨‹ä¸­ã€‚ æˆ‘ä»¬éå¸¸æ¬¢è¿å¤–éƒ¨å¼€å‘è€…ä¸ºPaddle Depthæä¾›æ–°åŠŸèƒ½\æ¨¡å‹ã€‚ å¦‚æœæ‚¨æœ‰è¿™æ–¹é¢çš„æ„æ„¿çš„è¯ï¼Œè¯·å¾€æˆ‘ä»¬çš„é‚®ç®±æˆ–è€…issueé‡Œé¢åé¦ˆ
## ğŸŒ€ æ„Ÿè°¢
PaddleDepth æ˜¯ä¸€æ¬¾ç”±æ¥è‡ªä¸åŒé«˜æ ¡å’Œä¼ä¸šçš„ç ”å‘äººå‘˜å…±åŒå‚ä¸è´¡çŒ®çš„å¼€æºé¡¹ç›®ã€‚
æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®æä¾›ç®—æ³•å¤ç°å’Œæ–°åŠŸèƒ½æ”¯æŒçš„è´¡çŒ®è€…ï¼Œä»¥åŠæä¾›å®è´µåé¦ˆçš„ç”¨æˆ·ã€‚ 
æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå·¥å…·ç®±å’ŒåŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºç¤¾åŒºæä¾›çµæ´»çš„ä»£ç å·¥å…·ï¼Œä¾›ç”¨æˆ·å¤ç°å·²æœ‰ç®—æ³•å¹¶å¼€å‘è‡ªå·±çš„æ–°æ¨¡å‹ï¼Œä»è€Œä¸æ–­ä¸ºå¼€æºç¤¾åŒºæä¾›è´¡çŒ®ã€‚

## å‚è€ƒæ–‡çŒ®

[1] Godard C, Mac Aodha O, Firman M, et al. Digging into self-supervised monocular depth estimation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 3828-3838.

[2] Song X, Li W, Zhou D, et al. MLDA-Net: Multi-level dual attention-based network for self-supervised monocular depth estimation[J]. IEEE Transactions on Image Processing, 2021, 30: 4691-4705.

[3] Watson J, Firman M, Brostow G J, et al. Self-supervised monocular depth hints[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 2162-2171.

[4] LEE J, HAN M, KO D, et al. From big to small: Multi-scale local planar guidance for monocular depth estimation[Z]//arXiv: Computer Vision and Pattern Recognition. 2019.

[comment]: <> "## Citation"

[comment]: <> "If you think this toolkit or the results are helpful to you and your research, please cite us!"

[comment]: <> "```"

[comment]: <> "@Misc{deepda,"

[comment]: <> "howpublished = {\url{https://github.com/jindongwang/transferlearning/tree/master/code/DeepDA}},   "

[comment]: <> "title = {DeepDA: Deep Domain Adaptation Toolkit},  "

[comment]: <> "author = {Wang, Jindong and Hou, Wenxin}"

[comment]: <> "}  "

[comment]: <> "```"



## è”ç³»æ–¹å¼

- [Yian Zhao](https://github.com/Zhao-Yian/): zhaoyian.zh@gmail.com
- [Zhelun Shen](https://github.com/gallenszl): shenzhelun@pku.edu.cn
- [Bopei Zheng](https://github.com/zbp-xxxp/): bopei.zheng@foxmail.com
